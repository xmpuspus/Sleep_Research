{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of stages: Light, Deep, REM, Wake Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database source: \n",
    "    \n",
    "    Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220 [Circulation Electronic Pages; http://circ.ahajournals.org/content/101/23/e215.full]; 2000 (June 13).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as ss\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import convolve1d\n",
    "import itertools\n",
    "import sleep_utils as su\n",
    "\n",
    "# import psycopg2\n",
    "import datetime  \n",
    "import time \n",
    "import wfdb\n",
    "import glob\n",
    "\n",
    "from biosppy.signals import ecg\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a file from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/PSG_database/'\n",
    "\n",
    "co2_sig = []\n",
    "ppg_sig = []\n",
    "\n",
    "filenames = []\n",
    "\n",
    "sleep_data_sigs = {}\n",
    "sleep_data_fields = {}\n",
    "\n",
    "data_keys = []\n",
    "for fname in glob.iglob(data_dir + '/*.dat'):#, recursive=True):\n",
    "    # filename manipulation\n",
    "    base = os.path.basename(fname)\n",
    "    key = os.path.splitext(base)[0]\n",
    "    data_keys.append(key)\n",
    "    \n",
    "    # load data\n",
    "    sig, fields=wfdb.rdsamp(data_dir+key)\n",
    "    sleep_data_sigs[key] = sig\n",
    "    sleep_data_fields[key] = fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ann_index = {}\n",
    "ann_labels = {}\n",
    "\n",
    "set_stages = set()\n",
    "for key in data_keys:\n",
    "    annotation = wfdb.rdann(data_dir + key, 'st')\n",
    "    ann_index_sig = annotation[0]\n",
    "    ann_label_sig = annotation[5]\n",
    "    label_filt = [(x, y) for x, y in zip(ann_index_sig, ann_label_sig) if x >1 ]\n",
    "    unzipped = zip(*label_filt)\n",
    "    ann_index[key] = unzipped[0]\n",
    "    ann_labels[key] = unzipped[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resp_data_set = {}\n",
    "ecg_data_set = {}\n",
    "time_data_set = {}\n",
    "annot_data_set = {}\n",
    "\n",
    "fs = 250\n",
    "dec_prec = len(str(1/fs).split('.')[1])\n",
    "\n",
    "for key in data_keys:\n",
    "    \n",
    "    resp_data_set[key] = (sleep_data_sigs[key])[:, (sleep_data_fields[key])['signame'].index('Resp')]\n",
    "    ecg_data_set[key] = (sleep_data_sigs[key])[:, (sleep_data_fields[key])['signame'].index('ECG')]\n",
    "    time_data_set[key] = np.arange(0, len(resp_data_set[key]))/fs\n",
    "\n",
    "    # Note: Only the first character of the annotation was taken as label\n",
    "    # Example: 1, 2, 3, 4, R, W, M\n",
    "    stages_list = [x.rsplit(' ')[0][0] for x in ann_labels[key]]\n",
    "\n",
    "    annot_data_set[key] = stages_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Annotations*\n",
    "\n",
    "Stages:\n",
    "   * W: subject is awake\n",
    "   * 1: sleep stage 1\n",
    "   * 2: sleep stage 2\n",
    "   * 3: sleep stage 3\n",
    "   * 4: sleep stage 4\n",
    "   * R: REM sleep\n",
    "   \n",
    "Other descriptions:\n",
    "   * H: Hypopnea\n",
    "   * HA: Hypopnea with arousal\n",
    "   * OA: Obstructive apnea\n",
    "   * X: Obstructive apnea with arousal\n",
    "   * CA: Central apnea\n",
    "   * CAA: Central apnea with arousal\n",
    "   * L: Leg movements\n",
    "   * LA: Leg movements with arousal\n",
    "   * A: Unspecified arousal\n",
    "   * MT: Movement time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Previous sleep stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Get features for every window of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "win_dur = 30 # duration of window, unit: seconds, size of one epoch \n",
    "win_size = fs*win_dur # length of window\n",
    "win_step = 1 # duration by which the window slides, unit: seconds\n",
    "win_int = win_step*fs # length by which the window slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for slp01a...\n",
      "Computing for slp01b...\n",
      "Computing for slp02a...\n",
      "Computing for slp02b...\n",
      "Computing for slp03...\n",
      "Computing for slp04...\n",
      "Computing for slp14...\n",
      "Computing for slp16...\n",
      "Computing for slp32...\n",
      "Computing for slp37...\n",
      "Computing for slp41...\n",
      "Computing for slp45...\n",
      "Computing for slp48...\n",
      "Computing for slp59...\n",
      "Computing for slp60...\n",
      "Computing for slp61...\n",
      "Computing for slp66...\n",
      "Computing for slp67x...\n"
     ]
    }
   ],
   "source": [
    "feature_prev_stage = {}\n",
    "\n",
    "for key in data_keys:\n",
    "    print(\"Computing for \"+ key + \"...\")\n",
    "\n",
    "    prev_stage = [np.nan] + annot_data_set[key][1:]\n",
    "    feature_prev_stage[key] = {'_feat' : prev_stage}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Respiration signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide respiration data per 30-second epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Respiratory frequency range \n",
    "min_normrange = 4/60 # unit: cycles per second\n",
    "max_normrange = 65/60 # unit: cycles per second\n",
    "\n",
    "resp_data_set_epochs = {}\n",
    "\n",
    "for key in data_keys:\n",
    "    resp_sig = resp_data_set[key]\n",
    "    fft_resp = np.fft.fft(resp_sig)\n",
    "    fft_freqs = np.fft.fftfreq(len(resp_sig), 1/fs)\n",
    "    \n",
    "    # Remove frequencies which are outside the expected range\n",
    "    fft_resp[abs(fft_freqs) < min_normrange] = 0\n",
    "    fft_resp[abs(fft_freqs) > max_normrange] = 0\n",
    "    \n",
    "    resp_filt_sig = np.real(np.fft.ifft(fft_resp))\n",
    "    time_sig = np.arange(len(resp_sig))/fs\n",
    "\n",
    "    # Divide into windows\n",
    "    resp_windows = su.divide_to_epochs(resp_filt_sig, ann_index[key], win_dur, fs)\n",
    "    time_windows = su.divide_to_epochs(time_sig, ann_index[key], win_dur, fs)\n",
    "    \n",
    "    resp_data_set_epochs[key] = pd.DataFrame({'resp': list(resp_windows), '_time': list(time_windows)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1 Respiration rate\n",
    "*Frequency corresponding to the highest peak in the epoch's power spectrum (unit: breaths/minute)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_resp_rate = {}\n",
    "# feature_resp_rate_fd = {}\n",
    "# feature_resp_rate_sd = {}\n",
    "\n",
    "for key in data_keys:\n",
    "    resp_data = resp_data_set_epochs[key]\n",
    "    resp_data['power_spectrum'] = resp_data.resp.apply(lambda x: ss.periodogram(x, fs=fs))\n",
    "    \n",
    "    # Compute respiration rate\n",
    "    resp_data['resp_rate'] = resp_data.power_spectrum.apply(lambda x: (x[0])[np.argmax(x[1])]*60)\n",
    "\n",
    "    feature_resp_rate[key] = {'_feat' : resp_data.resp_rate.values}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2 Histogram of Respiration data\n",
    "Use the counts for each bin of the magnitude histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins = 10\n",
    "\n",
    "feature_hist_resp = {}\n",
    "\n",
    "for key in data_keys:\n",
    "#     print(\"Computing for \"+ key + \"...\")\n",
    "    resp_data = resp_data_set_epochs[key]\n",
    "\n",
    "    hist_windows = resp_data.resp.apply(lambda x: np.histogram(x, bins=nbins)[0])\n",
    "    feature_hist_resp[key] = {'_feat' : (np.asarray(hist_windows))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "#### II.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### III. ECG signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide ECG data per 30-second epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecg_data_set_epochs = {}\n",
    "\n",
    "for key in data_keys:\n",
    "    ecg_sig = ecg_data_set[key]\n",
    "    time_sig = np.arange(len(ecg_sig))/fs\n",
    "\n",
    "    # Divide into windows\n",
    "    ecg_windows = su.divide_to_epochs(ecg_sig, ann_index[key], win_dur, fs)\n",
    "    time_windows = su.divide_to_epochs(time_sig, ann_index[key], win_dur, fs)\n",
    "    \n",
    "    ecg_data_set_epochs[key] = pd.DataFrame({'ecg': list(ecg_windows), '_time': list(time_windows)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1 Heart rate\n",
    "*Reciprocal of the mean R-R interval in an epoch (unit: beats/minute)*\n",
    "\n",
    "\n",
    "#### III.2 Heart rate variability\n",
    "*Standard deviation of the R-R intervals in an epoch (unit: milliseconds)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_heart_rate = {}\n",
    "feature_heart_rate_var = {}\n",
    "\n",
    "for key in data_keys:\n",
    "    ecg_data = ecg_data_set_epochs[key]\n",
    "    \n",
    "    # Get time corresponding to R peaks\n",
    "    ecg_data['r_peaks'] = ecg_data.ecg.apply(lambda x: ecg.hamilton_segmenter(x, sampling_rate=250)['rpeaks'])\n",
    "    ecg_data['r_time'] = ecg_data.apply(lambda x: list(x._time[x.r_peaks]), axis = 1)\n",
    "    \n",
    "    # Compute heart rate\n",
    "    ecg_data['heart_rate'] = ecg_data.r_time.apply(lambda x: su.heart_rate(np.array(x))*60)\n",
    "    ecg_data['window_time'] = ecg_data['_time'].apply(lambda x: x[-1])\n",
    "        \n",
    "    # Compute heart rate variability\n",
    "    ecg_data['heart_rate_var'] = ecg_data.r_time.apply(lambda x: su.heart_rate_var(np.array(x))*1000)\n",
    "    \n",
    "    feature_heart_rate[key] = {'_feat' : ecg_data.heart_rate.values}\n",
    "    feature_heart_rate_var[key] = {'_feat' : ecg_data.heart_rate_var.values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.3 Histogram of ECG data\n",
    "Use the counts for each bin of the magnitude histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_hist_ecg = {}\n",
    "\n",
    "for key in data_keys:\n",
    "#     print(\"Computing for \"+ key + \"...\")\n",
    "    ecg_data = ecg_data_set_epochs[key]\n",
    "\n",
    "    hist_windows = ecg_data.ecg.apply(lambda x: np.histogram(x, bins=nbins)[0])\n",
    "    feature_hist_ecg[key] = {'_feat' : (np.asarray(hist_windows))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compile features \n",
    "Combine features computed across all patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = [feature_prev_stage, feature_resp_rate, feature_hist_resp, \n",
    "            feature_heart_rate, feature_heart_rate_var, feature_hist_ecg]\n",
    "\n",
    "f_merge = []\n",
    "for f in features:\n",
    "    fm = []\n",
    "    for key in data_keys:\n",
    "        fm += list(f[key]['_feat'])\n",
    "    f_merge.append(np.reshape(fm, [len(fm), -1]))\n",
    "    \n",
    "sleep_features = np.hstack(f_merge)\n",
    "sleep_features[np.where(sleep_features == '1')] = 1\n",
    "sleep_features[np.where(sleep_features == '2')] = 2\n",
    "sleep_features[np.where(sleep_features == '3')] = 3\n",
    "sleep_features[np.where(sleep_features == '4')] = 4\n",
    "sleep_features[np.where(sleep_features == 'R')] = 5\n",
    "sleep_features[np.where(sleep_features == 'W')] = 6\n",
    "sleep_features[np.where(sleep_features == 'M')] = 7\n",
    "\n",
    "merge_labels = np.hstack([annot_data_set[key] for key in data_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sleep_features = np.array(sleep_features, dtype = float)\n",
    "sleep_data = sleep_features[np.isfinite(sleep_features).all(axis=1)]\n",
    "sleep_labels = np.reshape(merge_labels, [len(merge_labels), 1])[np.isfinite(sleep_features).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eventura/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sleep_data_scaled = StandardScaler().fit_transform(sleep_data)\n",
    "train_set, test_set, train_label, test_label = train_test_split(sleep_data_scaled, sleep_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\", \"Linear SVM\", \"RBF SVM\",]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(), \n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Nearest Neighbors\n",
      "('Nearest Neighbors', 0.81222056631892703)\n",
      "Training Decision Tree\n",
      "('Decision Tree', 1.0)\n",
      "Training Random Forest\n",
      "('Random Forest', 0.72727272727272729)\n",
      "Training AdaBoost\n",
      "('AdaBoost', 0.86825633383010437)\n",
      "Training Naive Bayes\n",
      "('Naive Bayes', 1.0)\n",
      "Training QDA\n",
      "('QDA', 0.99821162444113265)\n",
      "Training Linear SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eventura/anaconda/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Linear SVM', 0.9904619970193741)\n",
      "Training RBF SVM\n",
      "('RBF SVM', 0.50760059612518627)\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(len(classifiers)):\n",
    "    print('Training ' + names[i])\n",
    "    clf = classifiers[i]\n",
    "    clf.fit(train_set, np.ndarray.flatten(train_label))\n",
    "    prediction = clf.predict(test_set)\n",
    "    print(names[i], accuracy_score(test_label, prediction))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
